{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn_pandas import (\n",
    "    DataFrameMapper, \n",
    "    FunctionTransformer\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "feature_names = [col.replace(\" \", \"_\").replace(\"_(cm)\", \"\") \n",
    "                 for col in (iris[\"feature_names\"] + [\"species\"])]\n",
    "class_names = {0.0: \"setosa\", 1.0: \"versicolor\", 2.0: \"virginica\"}\n",
    "\n",
    "# convert to data frame purely for showing the data\n",
    "iris_df = pd.DataFrame(np.column_stack((X, y.reshape(-1, 1)))\n",
    "                       , columns=feature_names)\n",
    "\n",
    "# convert species from numeric to categorical\n",
    "iris_df[\"species\"] = iris_df[\"species\"].map(class_names)\n",
    "\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For educational purposes only, let's replace some values with `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random number generator\n",
    "random.seed(2019)\n",
    "\n",
    "# generate list of random integers \n",
    "random_ints = [random.randrange(0, len(iris_df)) for _ in range(20)]\n",
    "\n",
    "# for these random index values, replace their real values with NaN\n",
    "iris_df.loc[random_ints, \"sepal_length\"] = np.nan\n",
    "iris_df.loc[random_ints, \"sepal_width\"] = np.nan\n",
    "\n",
    "# manually force the first \"sepal_width\" value to also be NaN\n",
    "iris_df.loc[0, \"sepal_width\"] = np.nan\n",
    "\n",
    "iris_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function to identify records that have `NaN` values\n",
    "\n",
    "This will help us pick up on whether or not having a `NaN` value is helpful information in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_missing(x):\n",
    "    \"\"\"IDs records that contain missing (NaN) values\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not create an object that imputes median values\n",
    "\n",
    "Unfortunately, `sklearn_pandas` is still under development. The usage of the following:\n",
    "\n",
    "```python\n",
    "impute_median = SimpleImputer(missing_values=np.nan, strategy=\"median\")\n",
    "```\n",
    "\n",
    "inside `DataFrameMapper` will result in all columns that use the `impute_median` transformer to have the same value.\n",
    "\n",
    "Instead, do a gut check by manually printing out the median values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Median value for sepal length: {iris_df['sepal_length'].median()}\")\n",
    "print(f\"Median value for sepal width: {iris_df['sepal_width'].median()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store preprocessing steps in one object\n",
    "\n",
    "![kid transformer](visuals/transformer.gif)\n",
    "\n",
    "We'll store these steps in a `DataFrameMapper` object. A `DataFrameMapper` maps `pandas` data frame column subsets to their own transformations. This is abstraction is useful since we'll always be applying the same preprocessing steps to both our training and testing sets.\n",
    "\n",
    "The key here is we need to specify a specific column, pass it's \"transformer\" (i.e. `SimpleImputer`, `OneHotEncoder`, `StandardScaler`), and determinie if the transformation belongs in it's own new column or if it's more appropriate for the transformed column to overwrite the input column.\n",
    "\n",
    "Here, we'll create `mapper` object to do the following:\n",
    "\n",
    "* flag which records contain `NaN` values in the `sepal_length` feature;\n",
    "* impute the median value for `NaN` values in the `sepal_length` feature;\n",
    "* flag which records contain `NaN` values in the `sepal_width` feature;\n",
    "* impute the median value for `NaN` values in the `sepal_width` feature; and\n",
    "* return all other columns - `petal_length` & `petal_width` - as is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper(features=[\n",
    "    ([\"sepal_length\"], FunctionTransformer(is_missing), {\"alias\": \"sepal_length_missing\"}),\n",
    "    ([\"sepal_length\"], SimpleImputer(missing_values=np.nan, strategy=\"median\")),\n",
    "    ([\"sepal_width\"], FunctionTransformer(is_missing), {\"alias\": \"sepal_width_missing\"}),\n",
    "    ([\"sepal_width\"], SimpleImputer(missing_values=np.nan, strategy=\"median\")),\n",
    "],\n",
    "                         default=None,\n",
    "                         df_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just like a model object, \n",
    "# we'll fit and transform the DataFrameMapper object onto our data\n",
    "mapper.fit(iris_df).transform(iris_df).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the 9th record in both columns contains a value of `1`. This algins with the output of `iris_df.head(10)`, with both values in `sepal_length` and `sepal_width` containing `NaN` values.\n",
    "\n",
    "The use of our custom function was honored and bug-free, as only the first record in `sepal_width_missing` has a value of `1` and `sepal_length_missing` has a value of `0`.\n",
    "\n",
    "All records in the `petal_length`, `petal_width` and `species` columns are returned with no transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's take this up a level by performing `train_test_split` before any preprocessing and build a `DecisionTreeClassifier` model to classify which species a flower is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data \n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_df.drop(\"species\", axis=1),\n",
    "                                                    iris_df[\"species\"],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=2019,\n",
    "                                min_samples_leaf=30,\n",
    "                                criterion=\"gini\",\n",
    "                                min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"dataprep\", mapper),\n",
    "    (\"model\", dt_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit `X_train` and `y_train` onto the `pipe` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `pipe` object to make predictions on `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the first 10 predictions of our multi-class classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "* Make multi class ROC Curve: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get a general sense of accuracy\n",
    "\n",
    "* revist the accuracy score\n",
    "* introduce the confusion matrix\n",
    "    + Precision\n",
    "    + Recall\n",
    "    + F1 Score\n",
    "* introduce the ROC curve and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a confusion matrix to visualize where our model messed up\n",
    "\n",
    "> [A] confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one. - [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "# tn, fp, fn, tp\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build confusion matrix plot\n",
    "plt.figure(figsize=(6, 5), dpi=150)\n",
    "plt.imshow(cnf_matrix,  cmap=plt.cm.Blues) #Create the basic matrix.\n",
    "\n",
    "# Add title and Axis Labels\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "\n",
    "# add axis labels\n",
    "tick_marks = [val for val in class_names.keys()]\n",
    "plt.xticks(tick_marks, class_names.values())\n",
    "plt.yticks(tick_marks, class_names.values())\n",
    "\n",
    "# Add Labels to Each Cell\n",
    "thresh = cnf_matrix.max() / 2. #Used for text coloring below\n",
    "#Here we iterate through the confusion matrix and append labels to our visualization.\n",
    "for i in range(cnf_matrix.shape[0]):\n",
    "    for j in range(cnf_matrix.shape[1]):\n",
    "        plt.text(i, j, cnf_matrix[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "# Add a Side Bar Legend Showing Colors\n",
    "plt.colorbar()\n",
    "\n",
    "# Add padding\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot\n",
    "plt.savefig(\"visuals/multi_class_conf_matrix.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\")\n",
    "\n",
    "# display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the confusion matrix to evaluate our classifier\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy is the percentage of times that your model predicted correctly.\n",
    "\n",
    "Accuracy = correct predictions / total predictions\n",
    "\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual method\n",
    "correct_preds = 19 + 9 + 15\n",
    "total_preds = correct_preds + (1 + 1)\n",
    "print(f\"Accuracy score of {correct_preds / total_preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn.metrics method\n",
    "print(f\"Accuracy score of {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Of all the times your model predicted 1, how often was it correct?\n",
    "\n",
    "Precision = correct 1 predictions / correct 1 prediction + incorrect 1 prediction\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual method\n",
    "precision = {\"setosa\": 19 / (19 + 0),\n",
    "             \"versicolor\": 9 / (9 + 1),\n",
    "             \"virginica\": 15 / (15 + 1)}\n",
    "\n",
    "for key, val in precision.items():\n",
    "    print(f\"The precision score for {key} is {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn.metrics method\n",
    "# note: if the classifier was binary, we would leave 'average' set to 'binary'\n",
    "#       since this is a multi-class, we'll set the 'average' to None so we can get the precision score for each class\n",
    "for key, val in zip(precision.keys(), precision_score(y_test, y_pred, average=None)):\n",
    "    print(f\"The precision score for {key} is {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity aka Recall aka True Positive Rate (TPR)\n",
    "\n",
    "Sensitivity measures what percentage of (actual) 1â€™s your model correctly predicted.\n",
    "\n",
    "recall = correct 1 prediction / (correct 1 predictions + incorrect 0 predictions)\n",
    "\n",
    "$$ recall = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual method\n",
    "recall = {\"setosa\": 19 / (19 + 0),\n",
    "          \"versicolor\": 9 / (9 + 1),\n",
    "          \"virginica\": 15 / (15 + 1)}\n",
    "\n",
    "for key, val in recall.items():\n",
    "    print(f\"The recall score for {key} is {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn.metrics method\n",
    "# note: if the classifier was binary, we would leave 'average' set to 'binary'\n",
    "#       since this is a multi-class, we'll set the 'average' to None so we can get the recall score for each class\n",
    "for key, val in zip(recall.keys(), recall_score(y_test, y_pred, average=None)):\n",
    "    print(f\"The recall score for {key} is {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
